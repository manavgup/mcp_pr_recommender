"""Simple unit tests for PR recommender tools."""

from unittest.mock import Mock, patch

import pytest

from mcp_pr_recommender.tools.pr_recommender_tool import PRRecommenderTool
from mcp_pr_recommender.tools.strategy_manager_tool import StrategyManagerTool
from mcp_pr_recommender.tools.validator_tool import ValidatorTool


# Mock settings for all tests
@pytest.fixture(autouse=True)
def mock_settings():
    """Mock settings to avoid OpenAI API key requirement."""
    with patch("mcp_pr_recommender.config.settings") as mock_settings_func:
        mock_settings_instance = Mock()
        mock_settings_instance.openai_api_key = "test_key"
        mock_settings_instance.default_strategy = "semantic"
        mock_settings_instance.max_files_per_pr = 8
        mock_settings_instance.min_files_per_pr = 1
        mock_settings_instance.similarity_threshold = 0.7
        mock_settings_instance.enable_llm_analysis = True
        mock_settings_func.return_value = mock_settings_instance
        yield mock_settings_instance


@pytest.mark.unit
class TestPRRecommenderToolSimple:
    """Simple test cases for the PR recommender tool."""

    def test_tool_initialization(self):
        """Test tool can be initialized."""
        with patch("openai.AsyncOpenAI"), patch(
            "mcp_pr_recommender.services.semantic_analyzer.settings"
        ) as mock_sa_settings:
            mock_sa_settings.return_value.openai_api_key = "test_key"
            tool = PRRecommenderTool()
            assert tool is not None
            assert hasattr(tool, "semantic_analyzer")

    @pytest.mark.asyncio
    async def test_generate_recommendations_empty_data(self):
        """Test handling of empty analysis data."""
        with patch("openai.AsyncOpenAI"), patch(
            "mcp_pr_recommender.services.semantic_analyzer.settings"
        ) as mock_sa_settings:
            mock_sa_settings.return_value.openai_api_key = "test_key"
            tool = PRRecommenderTool()

            # Mock the semantic analyzer
            with patch.object(tool, "semantic_analyzer") as mock_analyzer:
                mock_analyzer.generate_pr_recommendations.return_value = {
                    "recommendations": [],
                    "metadata": {"reason": "No changes detected"},
                }

                result = await tool.generate_recommendations({})

                assert "recommendations" in result
                # Tool returns different structure - check for key fields
                assert "strategy_used" in result or "error" in result


@pytest.mark.unit
class TestStrategyManagerToolSimple:
    """Simple test cases for the strategy manager tool."""

    def test_tool_initialization(self):
        """Test tool can be initialized."""
        tool = StrategyManagerTool()
        assert tool is not None
        assert hasattr(tool, "logger")

    @pytest.mark.asyncio
    async def test_get_strategies(self, mock_settings):
        """Test getting available strategies."""
        tool = StrategyManagerTool()

        with patch(
            "mcp_pr_recommender.tools.strategy_manager_tool.settings"
        ) as mock_runtime_settings:
            mock_runtime_settings.return_value.openai_api_key = "test_key"
            mock_runtime_settings.return_value.default_strategy = "semantic"
            mock_runtime_settings.return_value.max_files_per_pr = 8
            mock_runtime_settings.return_value.similarity_threshold = 0.7

            result = await tool.get_strategies()

        assert "available_strategies" in result
        assert "default_strategy" in result
        assert "current_settings" in result
        assert "recommendations" in result

        # Verify expected strategies are present
        strategies = result["available_strategies"]
        expected_strategies = ["semantic", "directory", "size", "dependency", "hybrid"]
        for strategy in expected_strategies:
            assert strategy in strategies


@pytest.mark.unit
class TestValidatorToolSimple:
    """Simple test cases for the validator tool."""

    def test_tool_initialization(self):
        """Test tool can be initialized."""
        tool = ValidatorTool()
        assert tool is not None
        assert hasattr(tool, "logger")

    @pytest.mark.asyncio
    async def test_validate_empty_recommendations(self):
        """Test validation of empty recommendations."""
        tool = ValidatorTool()

        result = await tool.validate_recommendations([])

        assert "overall_valid" in result
        assert "recommendations_analysis" in result
        assert "coverage_analysis" in result
        assert "conflict_analysis" in result
        assert "suggestions" in result
        assert "quality_score" in result

        assert result["overall_valid"] is True  # Empty is valid
        assert len(result["recommendations_analysis"]) == 0
        assert result["quality_score"] == 0.0

    @pytest.mark.asyncio
    async def test_validate_single_valid_recommendation(self):
        """Test validation of single valid recommendation."""
        tool = ValidatorTool()

        valid_rec = [
            {
                "id": "pr_001",
                "title": "Add authentication system",
                "description": "Implement user authentication with login and logout",
                "files": [
                    "src/auth/login.py",
                    "src/auth/logout.py",
                    "tests/test_auth.py",
                ],
                "estimated_size": "medium",
                "priority": "high",
                "risk_level": "medium",
                "confidence": 0.9,
                "labels": ["feature", "authentication"],
            }
        ]

        with patch(
            "mcp_pr_recommender.tools.validator_tool.settings"
        ) as mock_runtime_settings:
            mock_runtime_settings.return_value.openai_api_key = "test_key"
            mock_runtime_settings.return_value.max_files_per_pr = 8
            mock_runtime_settings.return_value.min_files_per_pr = 1

            result = await tool.validate_recommendations(valid_rec)

        assert len(result["recommendations_analysis"]) == 1
        # Note: The actual validation logic may mark this as invalid due to
        # missing fields or other validation rules, so we just check structure
        assert "valid" in result["recommendations_analysis"][0]
        # Tool returns "issues" not "errors"
        assert "issues" in result["recommendations_analysis"][0]


@pytest.mark.unit
class TestToolIntegration:
    """Test basic integration between tools."""

    def test_all_tools_can_be_imported(self):
        """Test that all tool classes can be imported and initialized."""
        with patch("openai.AsyncOpenAI"), patch(
            "mcp_pr_recommender.services.semantic_analyzer.settings"
        ) as mock_sa_settings:
            mock_sa_settings.return_value.openai_api_key = "test_key"
            pr_tool = PRRecommenderTool()
            strategy_tool = StrategyManagerTool()
            validator_tool = ValidatorTool()

            assert pr_tool is not None
            assert strategy_tool is not None
            assert validator_tool is not None

    @pytest.mark.asyncio
    async def test_workflow_simulation(self, mock_settings):
        """Test a basic workflow simulation."""
        with patch("openai.AsyncOpenAI"), patch(
            "mcp_pr_recommender.services.semantic_analyzer.settings"
        ) as mock_sa_settings:
            mock_sa_settings.return_value.openai_api_key = "test_key"
            # Initialize tools
            strategy_tool = StrategyManagerTool()
            pr_tool = PRRecommenderTool()
            validator_tool = ValidatorTool()

            # Get strategies - need runtime settings mocking
            with patch(
                "mcp_pr_recommender.tools.strategy_manager_tool.settings"
            ) as mock_runtime_settings:
                mock_runtime_settings.return_value.openai_api_key = "test_key"
                mock_runtime_settings.return_value.default_strategy = "semantic"
                mock_runtime_settings.return_value.max_files_per_pr = 8
                mock_runtime_settings.return_value.similarity_threshold = 0.7

                strategies_result = await strategy_tool.get_strategies()
                assert "available_strategies" in strategies_result

            # Generate recommendations (mocked)
            with patch.object(pr_tool, "semantic_analyzer") as mock_analyzer:
                mock_analyzer.generate_pr_recommendations.return_value = {
                    "recommendations": [
                        {
                            "id": "pr_001",
                            "title": "Test PR",
                            "description": "Test changes",
                            "files": ["src/test.py"],
                            "estimated_size": "small",
                            "priority": "medium",
                            "risk_level": "low",
                            "confidence": 0.8,
                        }
                    ],
                    "metadata": {"strategy": "semantic"},
                }

                pr_result = await pr_tool.generate_recommendations({})
                assert "recommendations" in pr_result

                # Validate recommendations
                recommendations = pr_result["recommendations"]
                validation_result = await validator_tool.validate_recommendations(
                    recommendations
                )
                assert "overall_valid" in validation_result
